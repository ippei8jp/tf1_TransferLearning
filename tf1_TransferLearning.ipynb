{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf1_TransferLearning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ksCkbvgvr5T"
      },
      "source": [
        "以下の参考サイトの手順をTensorflow1で実行するように変更した。  \n",
        "参考：  https://towardsdatascience.com/train-an-object-detector-using-tensorflow-2-object-detection-api-in-2021-a4fed450d1b9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Im7HFPqp9DSD"
      },
      "source": [
        "# 事前準備\n",
        "ランタイム→ランタイムのタイプを変更を選択、ハードウェアアクセラレータを「GPU」に変更しておく。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7p-XPLJqQjnx"
      },
      "source": [
        "# Tensorflowのバージョンを1.xに変更する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YVcdijWNqph"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ishd8f-vuvxl"
      },
      "source": [
        "【NOTE】  \n",
        "以下でTensorflowのバージョンを変更しようとすると、なぜかCPU版になってしまう。\n",
        "``tensorflow-gpu``だとtensorflowが見つからないと怒られる。  \n",
        "```\n",
        "!pip uninstall -y tensorflow\n",
        "!pip install --upgrade tensorflow-gpu==1.15.5\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFMEGhWS8Grv"
      },
      "source": [
        "# ターゲットの指定\n",
        "\n",
        "HAND: 手の認識  \n",
        "それ以外：Pascal VOC  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClWu8f-r8Ry5"
      },
      "source": [
        "TARGET_DATA=\"HAND\"\n",
        "# TARGET_DATA=\"VOC\"\n",
        "print(TARGET_DATA)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2etjP2Rou_b"
      },
      "source": [
        "# GoogleDriveのマウント\n",
        "中断後の処理再開に必要なファイルをGoogleDrive上にファイルを保存するため、GoogleDriveをマウントする。    \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxOSwlbkonaH"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# ベースディレクトリ\n",
        "BASE_DIR = \"/content\"\n",
        "\n",
        "if TARGET_DATA == \"HAND\" :\n",
        "  # ワークディレクトリ\n",
        "  WORK_DIR = '/content/drive/MyDrive/hand_detect_tf1'\n",
        "else :\n",
        "  # ワークディレクトリ\n",
        "  WORK_DIR = '/content/drive/MyDrive/voc_detect_tf1'\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive\n",
        "\n",
        "if os.path.exists(WORK_DIR) :\n",
        "  # 2回目以降の実行\n",
        "  FIRST_EXEC=False\n",
        "  print(\"2nd ececution\")\n",
        "else :\n",
        "  # 最初の実行\n",
        "  FIRST_EXEC=True\n",
        "  !mkdir -p $WORK_DIR\n",
        "  print(\"1st ececution\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGgG286KpXYt"
      },
      "source": [
        "# object-detection モジュールのインストール\n",
        "## gitリポジトリのclone ～ インストール"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoxRr0fxpjUQ"
      },
      "source": [
        "%cd $BASE_DIR\n",
        "!git clone --depth 1 https://github.com/tensorflow/models.git\n",
        "\n",
        "# プロトコルバッファのコンパイル\n",
        "%cd models/research\n",
        "!protoc object_detection/protos/*.proto --python_out=.\n",
        "\n",
        "# モジュールのインストール\n",
        "!cp object_detection/packages/tf1/setup.py .\n",
        "!python -m pip install .\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqXTho4itAr5"
      },
      "source": [
        "## テスト"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2AO1753rlr1"
      },
      "source": [
        "%cd $BASE_DIR/models/research\n",
        "!python object_detection/builders/model_builder_tf1_test.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3qxMI7PwvAW"
      },
      "source": [
        "# データセットのダウンロード\n",
        "## gitリポジトリのclone→csvファイル→tf_recorfファイル"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1sv0LdLtHHt"
      },
      "source": [
        "LABEL_MAP = os.path.join(WORK_DIR, \"label_map.pbtxt\")\n",
        "TRAIN_FILE = os.path.join(WORK_DIR, \"train.record\")\n",
        "TEST_FILE  = os.path.join(WORK_DIR, \"test.record\")\n",
        "\n",
        "if TARGET_DATA == \"HAND\" :\n",
        "\n",
        "  if FIRST_EXEC :\n",
        "    %cd $BASE_DIR\n",
        "    !git clone https://github.com/aalpatya/detect_hands.git\n",
        "    !python detect_hands/egohands_dataset_to_csv.py\n",
        "    !python detect_hands/generate_tfrecord.py --csv_input=images/train/train_labels.csv  --output_path=$TRAIN_FILE\n",
        "    !python detect_hands/generate_tfrecord.py --csv_input=images/test/test_labels.csv    --output_path=$TEST_FILE\n",
        "\n",
        "    # ラベルファイルをコピー\n",
        "    ! cp ./detect_hands/model_data/ssd_mobilenet_v2_fpn_320/label_map.pbtxt $LABEL_MAP\n",
        "  else :\n",
        "      print(\"2nd ececution\")\n",
        "\n",
        "else :\n",
        "  if FIRST_EXEC :\n",
        "    %cd $BASE_DIR\n",
        "    # !wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar -O - | tar xvf -\n",
        "    # ミラーサイト使用の場合はこちら\n",
        "    !wget http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar  -O - | tar xvf -\n",
        "    %cd $BASE_DIR/models/research/object_detection/\n",
        "    !cp ./data/pascal_label_map.pbtxt $LABEL_MAP\n",
        "    !python dataset_tools/create_pascal_tf_record.py --label_map_path $LABEL_MAP --data_dir $BASE_DIR/VOCdevkit --year VOC2007 --set train --output_path $TRAIN_FILE\n",
        "    !python dataset_tools/create_pascal_tf_record.py --label_map_path $LABEL_MAP --data_dir $BASE_DIR/VOCdevkit --year VOC2007 --set val   --output_path $TEST_FILE\n",
        "\n",
        "  else :\n",
        "      print(\"2nd ececution\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xl6mI4uA08MZ"
      },
      "source": [
        "# 元となるモデルのダウンロード\n",
        "\n",
        "元になるモデルファイルは以下を参照  \n",
        "https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf1_detection_zoo.md\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SO_X6UycyinH"
      },
      "source": [
        "# 学習済みモデル出力ディレクトリ\n",
        "TRAINED_DIR_REL = \"output_training\"\n",
        "TRAINED_DIR = os.path.join(WORK_DIR, TRAINED_DIR_REL)\n",
        "\n",
        "# CONFIGファイル名\n",
        "CONFIG_FILE     = os.path.join(WORK_DIR, \"pipeline.config\")\n",
        "\n",
        "if FIRST_EXEC :\n",
        "  %cd $WORK_DIR\n",
        "  \n",
        "  # 元となるモデルのダウンロード\n",
        "  !wget http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_coco_2018_03_29.tar.gz -O - | tar xzvf -\n",
        "\n",
        "  BASE_MODEL_DIR = os.path.join(WORK_DIR, \"ssd_mobilenet_v2_coco_2018_03_29\")\n",
        "  # 元となるモデルのCHECKPOINTファイル\n",
        "  CKPT_FILE   =  os.path.join(BASE_MODEL_DIR, \"checkpoint/ckpt-0\")\n",
        "  # 元となるモデルのconfigファイル\n",
        "  CONFIG_FILE_ORG = os.path.join(BASE_MODEL_DIR, \"pipeline.config\")\n",
        "\n",
        "  # 作業用CONFIGファイルを作成\n",
        "  !cp $CONFIG_FILE_ORG $CONFIG_FILE\n",
        "\n",
        "  if TARGET_DATA == \"HAND\" :\n",
        "    # 変更パラメータ\n",
        "    NUM_CLASSES = 1                   # クラス数\n",
        "    # NUM_STEPS = 200000                # 回数は要検討\n",
        "    BATCH_SIZE = 4                    # バッチサイズ\n",
        "  else :\n",
        "    # 変更パラメータ\n",
        "    NUM_CLASSES = 20                  # クラス数\n",
        "    # NUM_STEPS = 100000                # 回数は要検討\n",
        "    BATCH_SIZE = 64                   # バッチサイズ\n",
        "\n",
        "  import re\n",
        "  from object_detection.protos import pipeline_pb2\n",
        "  from google.protobuf import text_format\n",
        "  import tensorflow.compat.v1 as tf\n",
        "\n",
        "  # CONFIGファイル読み込み\n",
        "  pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n",
        "  with tf.gfile.GFile(CONFIG_FILE, \"r\") as f:\n",
        "    proto_str = f.read()\n",
        "\n",
        "    # batch_norm_trainable エントリを削除(エラー回避)\n",
        "    proto_str = re.sub('batch_norm_trainable:.*\\\\n', '',  proto_str)      \n",
        "    \n",
        "    text_format.Merge(proto_str, pipeline_config)\n",
        "\n",
        "  # パラメータ変更\n",
        "  pipeline_config.model.ssd.num_classes                                      = NUM_CLASSES\n",
        "\n",
        "  pipeline_config.train_config.batch_size                                    = BATCH_SIZE\n",
        "  pipeline_config.train_config.fine_tune_checkpoint                          = CKPT_FILE\n",
        "  # num_steps はコマンドラインで変更するのでここでは省略\n",
        "  # pipeline_config.train_config.num_steps                                     = NUM_STEPS\n",
        "  pipeline_config.train_config.fine_tune_checkpoint_type                     = \"detection\"\n",
        "\n",
        "  pipeline_config.train_input_reader.label_map_path                          = LABEL_MAP\n",
        "  pipeline_config.train_input_reader.tf_record_input_reader.input_path[0]    = TRAIN_FILE\n",
        "\n",
        "  pipeline_config.eval_input_reader[0].label_map_path                        = LABEL_MAP\n",
        "  pipeline_config.eval_input_reader[0].tf_record_input_reader.input_path[0]  = TEST_FILE\n",
        "\n",
        "  # momentum_optimizerに変えてみたらうまく動かなかった(T_T)\n",
        "  \"\"\"\n",
        "  # optimizerパラメータをバッサリ変更\n",
        "  # delattr(pipeline_config.train_config.optimizer, rms_prop_optimizer)\n",
        "  pipeline_config.train_config.optimizer.momentum_optimizer.learning_rate.cosine_decay_learning_rate.learning_rate_base   = 0.08\n",
        "  pipeline_config.train_config.optimizer.momentum_optimizer.learning_rate.cosine_decay_learning_rate.total_steps          = NUM_STEPS\n",
        "  pipeline_config.train_config.optimizer.momentum_optimizer.learning_rate.cosine_decay_learning_rate.warmup_learning_rate = 0.026666\n",
        "  pipeline_config.train_config.optimizer.momentum_optimizer.learning_rate.cosine_decay_learning_rate.warmup_steps         = 1000\n",
        "  pipeline_config.train_config.optimizer.momentum_optimizer.momentum_optimizer_value: 0.9\n",
        "  pipeline_config.train_config.optimizer.use_moving_average = False\n",
        "  \"\"\"\n",
        "  \n",
        "  # 変更後データの書き込み\n",
        "  pipeline_text = text_format.MessageToString(pipeline_config)\n",
        "  with tf.gfile.Open(CONFIG_FILE, \"wb\") as f:\n",
        "    f.write(pipeline_text)\n",
        "else :\n",
        "    print(\"2nd ececution\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5hqGiXmVRy3"
      },
      "source": [
        "# 学習の実行\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQ8SGxxssP4U"
      },
      "source": [
        "## ちょこっと実行"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgLESxOPVXHx"
      },
      "source": [
        "%cd $BASE_DIR/models/research/object_detection/\n",
        "\n",
        "# 学習回数(エポック数)\n",
        "if TARGET_DATA == \"HAND\" :\n",
        "  # 変更パラメータ\n",
        "  NUM_STEPS = 200000                 # 回数は要検討\n",
        "else :\n",
        "  # NUM_STEPS = 100000                 # 回数は要検討\n",
        "  NUM_STEPS = 50000                 # 回数は要検討\n",
        "\n",
        "# 最初の実行のときにtensorboardがうまく表示されないので、ちょっとだけ回しておく\n",
        "if FIRST_EXEC :\n",
        "  !python model_main.py \\\n",
        "  --pipeline_config_path=$CONFIG_FILE \\\n",
        "  --model_dir=$TRAINED_DIR \\\n",
        "  --alsologtostderr \\\n",
        "  --num_train_steps=1000\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qsBR_waUmwp"
      },
      "source": [
        "## TensorBoardの起動\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZcQeTHNUrSp"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=$TRAINED_DIR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K07thTJesDRq"
      },
      "source": [
        "## 本番実行"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADsU-0_5Mjbs"
      },
      "source": [
        "# 本番の学習\n",
        "!python model_main.py \\\n",
        "--pipeline_config_path=$CONFIG_FILE \\\n",
        "--model_dir=$TRAINED_DIR \\\n",
        "--alsologtostderr \\\n",
        "--num_train_steps=$NUM_STEPS\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prx67pKuywa4"
      },
      "source": [
        "# モデルのエクスポート\n",
        "モデルをエクスポートしてSavedModelを作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z98Yb4miy0dp"
      },
      "source": [
        "%cd $BASE_DIR/models/research/object_detection\n",
        "\n",
        "if TARGET_DATA == \"HAND\" :\n",
        "  EXPORT_DIR_REL=\"hand_detect\"\n",
        "else:\n",
        "  EXPORT_DIR_REL=\"voc_detect\"\n",
        "\n",
        "EXPORT_DIR=os.path.join(WORK_DIR, EXPORT_DIR_REL)\n",
        "\n",
        "# エクスポート先が存在しているとエラーになるので削除しておく\n",
        "!rm -fr $EXPORT_DIR\n",
        "\n",
        "CHKPT_PREFIX = f\"{TRAINED_DIR}/model.ckpt-{NUM_STEPS}\"\n",
        "\n",
        "!python export_inference_graph.py \\\n",
        "--input_type image_tensor \\\n",
        "--pipeline_config_path $CONFIG_FILE \\\n",
        "--trained_checkpoint_prefix $CHKPT_PREFIX \\\n",
        "--output_directory $EXPORT_DIR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7_TOsgbDh7c"
      },
      "source": [
        "エクスポートしたデータをアーカイブ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcfTJS4Ysp87"
      },
      "source": [
        "%cd $WORK_DIR\n",
        "# ラベルファイルもコピー\n",
        "!cp $LABEL_MAP $EXPORT_DIR_REL\n",
        "\n",
        "# ラベルファイルをテキストファイル化しておく\n",
        "# ===================================\n",
        "import sys\n",
        "import os\n",
        "import object_detection.utils.label_map_util as label_util\n",
        "\n",
        "INPUT_LABELS_FILE  = LABEL_MAP\n",
        "OUTPUT_LABELS_FILE = os.path.join(EXPORT_DIR_REL, \"label_map.txt\")\n",
        "\n",
        "category_index = label_util.create_category_index_from_labelmap(INPUT_LABELS_FILE)\n",
        "\n",
        "with open(OUTPUT_LABELS_FILE, mode='w') as f:\n",
        "    for i in range(len(category_index)+1) :\n",
        "        try:\n",
        "            name = category_index[i][\"name\"]\n",
        "        except:\n",
        "            name = str(i)\n",
        "        \n",
        "        # print(name)\n",
        "        f.write(name + '\\n')\n",
        "# ===================================\n",
        "\n",
        "import datetime\n",
        "# 現在時刻(タイムゾーン情報付加)\n",
        "now = datetime.datetime.now().astimezone(datetime.timezone(datetime.timedelta(hours=+9)))\n",
        "# ZIPファイル名を生成\n",
        "zip_filename_export  = now.strftime(f'{EXPORT_DIR_REL}_{NUM_STEPS}_%Y%m%d_%H%M%S.zip')\n",
        "zip_filename_trained = now.strftime(f'{TRAINED_DIR_REL}_{NUM_STEPS}_%Y%m%d_%H%M%S.zip')\n",
        "\n",
        "!zip -r $zip_filename_export  $EXPORT_DIR_REL\n",
        "!zip -r $zip_filename_trained $TRAINED_DIR_REL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWhHVYk5h4Aq"
      },
      "source": [
        "# テスト"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eu9AyJI3h88r"
      },
      "source": [
        "%cd $BASE_DIR\n",
        "# テスト用画像ファイルのダウンロード\n",
        "if TARGET_DATA == \"HAND\" :\n",
        "  !wget https://cdn.amebaowndme.com/madrid-prd/madrid-web/images/sites/483796/1357355de6edbc4c4b54d22faf0b0756_ce052e9b134a9dbb047a8e17c890832a.jpg -O a.jpg\n",
        "  !wget https://cdn.amebaowndme.com/madrid-prd/madrid-web/images/sites/483796/564b6ca69e9022aa1977f335a148a05a_2d642c807aaf8f5b972a0a406903447d.jpg -O b.jpg\n",
        "else :\n",
        "  !wget https://prtimes.jp/i/6067/298/resize/d6067-298-418042-0.jpg -O a.jpg\n",
        "  !wget https://www.kic-car.ac.jp/theme/kic_school/img/taisho/ph-society001.jpg -O b.jpg\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZ75krjCiT0d"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import cv2\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "\n",
        "from object_detection.utils import ops as utils_ops\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as vis_util\n",
        "\n",
        "# patch tf1 into `utils.ops`\n",
        "utils_ops.tf = tf.compat.v1\n",
        "\n",
        "# Patch the location of gfile\n",
        "tf.gfile = tf.io.gfile\n",
        "\n",
        "# ラベルマップのロード\n",
        "PATH_TO_LABELS =  LABEL_MAP\n",
        "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)\n",
        "\n",
        "# テスト用イメージファイル\n",
        "TEST_IMAGE_PATHS = [\n",
        "                        \"a.jpg\", \n",
        "                        \"b.jpg\",\n",
        "                    ]\n",
        "\n",
        "# モデルのロード\n",
        "PATH_TO_FROZEN_GRAPH = os.path.join(EXPORT_DIR, \"frozen_inference_graph.pb\")\n",
        "detection_graph = tf.Graph()\n",
        "with detection_graph.as_default():\n",
        "  od_graph_def = tf.GraphDef()\n",
        "  with tf.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid:\n",
        "    serialized_graph = fid.read()\n",
        "    od_graph_def.ParseFromString(serialized_graph)\n",
        "    tf.import_graph_def(od_graph_def, name='')\n",
        "\n",
        "\n",
        "def load_image_into_numpy_array(image):\n",
        "  (im_width, im_height) = image.size\n",
        "  return np.array(image.getdata()).reshape(\n",
        "      (im_height, im_width, 3)).astype(np.uint8)\n",
        "\n",
        "\n",
        "def run_inference_for_single_image(image, graph):\n",
        "  with graph.as_default():\n",
        "    with tf.Session() as sess:\n",
        "      # Get handles to input and output tensors\n",
        "      ops = tf.get_default_graph().get_operations()\n",
        "      all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
        "      tensor_dict = {}\n",
        "      for key in [\n",
        "          'num_detections', 'detection_boxes', 'detection_scores',\n",
        "          'detection_classes', 'detection_masks'\n",
        "      ]:\n",
        "        tensor_name = key + ':0'\n",
        "        if tensor_name in all_tensor_names:\n",
        "          tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
        "              tensor_name)\n",
        "      if 'detection_masks' in tensor_dict:\n",
        "        # The following processing is only for single image\n",
        "        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
        "        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
        "        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
        "        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
        "        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
        "        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
        "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
        "            detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
        "        detection_masks_reframed = tf.cast(\n",
        "            tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
        "        # Follow the convention by adding back the batch dimension\n",
        "        tensor_dict['detection_masks'] = tf.expand_dims(\n",
        "            detection_masks_reframed, 0)\n",
        "      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
        "\n",
        "      # Run inference\n",
        "      output_dict = sess.run(tensor_dict,\n",
        "                             feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
        "\n",
        "      # all outputs are float32 numpy arrays, so convert types as appropriate\n",
        "      output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
        "      output_dict['detection_classes'] = output_dict[\n",
        "          'detection_classes'][0].astype(np.uint8)\n",
        "      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
        "      output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
        "      if 'detection_masks' in output_dict:\n",
        "        output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
        "  return output_dict\n",
        "\n",
        "def show_inference(image_path):\n",
        "  # 画像の読み込み\n",
        "  image = Image.open(image_path)\n",
        "  image_np = load_image_into_numpy_array(image)\n",
        "  image_np_expanded = np.expand_dims(image_np, axis=0)\n",
        "\n",
        "  output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
        "  # Visualization of the results of a detection.\n",
        "  vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "      image_np,\n",
        "      output_dict['detection_boxes'],\n",
        "      output_dict['detection_classes'],\n",
        "      output_dict['detection_scores'],\n",
        "      category_index,\n",
        "      instance_masks=output_dict.get('detection_masks'),\n",
        "      use_normalized_coordinates=True,\n",
        "      line_thickness=8)\n",
        "  # plt.figure(figsize=IMAGE_SIZE)\n",
        "  # plt.imshow(image_np)\n",
        "\n",
        "  # 表示\n",
        "  display(Image.fromarray(image_np))\n",
        "  # ～～～ 単独実行するときの表示処理はこちら ～～～\n",
        "  # new_image = cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR)\n",
        "  # cv2.imshow(\"Detection Results\", new_image)  \n",
        "  # cv2.waitKey(0)\n",
        "  # cv2.destroyAllWindows()\n",
        "\n",
        "# 実行\n",
        "for image_path in TEST_IMAGE_PATHS:\n",
        "  show_inference(image_path)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}